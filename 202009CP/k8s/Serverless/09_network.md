#### Network stack

“网络栈”，就包括了:网卡(Network Interface)、回环设备(Loopback Device)、路由表(Routing Table)和 iptables 规则。对于一个进程来说，这些要素，其实 就构成了它发起和响应网络请求的基本环境。

作为一个容器，它可以声明直接使用宿主机的网络栈(–net=host)，即:不开 启 Network Namespace，即：
**docker run -d -net=host --name nginx-host nginx**

这个容器启动后，直接监听的就是宿主机的 80 端口。

在大多数情况下，我们都希望容器进程能使用 自己 Network Namespace 里的网络栈，即:拥有属于自己的 IP 地址和端口。
这时候，一个显而易见的问题就是:这个被隔离的容器进程，该如何跟其他 Network Namespace 里的容器进程进行交互呢?

**可以把每一个容器看做一台主机，它们都有一套独立的“网络栈”。**

在 Linux 中，能够起到虚拟交换机作用的网络设备，是网桥(Bridge)。它是一个工作在数据 链路层(Data Link)的设备，主要功能是根据 MAC 地址学习来将数据包转发到网桥的不同端 口(Port)上。而为了实现上述目的，Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连 接在 docker0 网桥上的容器，就可以通过它来进行通信。


#### Veth pair

Veth Pair 设备的特点是:它被创建出来后，总是以两张虚拟网卡(Veth Peer)的形式成对出 现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网 卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里。这就使得 Veth Pair 常常被用作连接不同 Network Namespace 的“网线”。

在默认情况下，被限制在 Network Namespace 里的容器进程，实际上是通过 Veth Pair 设备 + 宿主机网桥的方式，实现了跟同 其他容器的数据交换。


当一个容器试图连接到另外一个宿主机时，比如:ping 10.168.0.3，它发出的请求数 据包，首先经过 docker0 网桥出现在宿主机上。然后根据宿主机的路由表里的直连路由规则 (10.168.0.0/24 via eth0))，对 10.168.0.3 的访问请求就会交给宿主机的 eth0 处理。
所以接下来，这个数据包就会经宿主机的 eth0 网卡转发到宿主机网络上，最终到达 10.168.0.3 对应的宿主机上。当然，这个过程的实现要求这两台宿主机本身是连通的。


#### Overlay network

如果在另外一台宿主机(比如:10.168.0.3)上，也有一个 Docker 容器。那么，我们的 nginx-1 容器又该如何访问它呢?

在 Docker 的默认配置下，一台宿主机上的 docker0 网桥，和其他宿主机上的 docker0 网桥， 没有任何关联，它们互相之间也没办法连通。所以，连接在这些网桥上的容器，自然也没办法进 行通信了。

通过软件的方式，创建一个整个集群“公用”的网桥，然后把集群里的所有容器都连接 到这个网桥上，就可以相互通信了.

构建这种容器网络的核心在于:我们需要在已有的宿主机网络上，再通过软件构建一 个覆盖在已有宿主机网络之上的、可以把所有容器连通在一起的虚拟网络。所以，这种技术就被 称为:Overlay Network(覆盖网络)。

而这个 Overlay Network 本身，可以由每台宿主机上的一个“特殊网桥”共同组成。比如，当 Node 1 上的 Container 1 要访问 Node 2 上的 Container 3 的时候，Node 1 上的“特殊网 桥”在收到数据包之后，能够通过某种方式，把数据包发送到正确的宿主机，比如 Node 2上。而 Node 2 上的“特殊网桥”在收到数据包后，也能够通过某种方式，把数据包转发给正 确的容器，比如 Container 3。
甚至，每台宿主机上，都不需要有一个这种特殊的网桥，而仅仅通过某种方式配置宿主机的路由 表，就能够把数据包转发到正确的宿主机上。



#### Flannel

Flannel 项目是 CoreOS 公司主推的容器网络方案。事实上，Flannel 项目本身只是一个框架，真 正为我们提供容器网络功能的，是 Flannel 的后端实现。目前，Flannel 支持三种后端实现，分别 是:
1. VXLAN;
2. host-gw;
3. UDP。


在这个例子中，我有两台宿主机。
* 宿主机 Node 1 上有一个容器 container-1，它的 IP 地址是 100.96.1.2，对应的 docker0 网桥 的地址是:100.96.1.1/24。
* 宿主机 Node 2 上有一个容器 container-2，它的 IP 地址是 100.96.2.3，对应的 docker0 网桥 的地址是:100.96.2.1/24。

现在的任务，就是让 container-1 访问 container-2。

这种情况下，container-1 容器里的进程发起的 IP 包，其源地址就是 100.96.1.2，目的地址就是 100.96.2.3。由于目的地址 100.96.2.3 并不在 Node 1 的 docker0 网桥的网段里，所以这个 IP 包 会被交给默认路由规则，通过容器的网关进入 docker0 网桥(如果是同一台宿主机上的容器间通 信，走的是直连规则)，从而出现在宿主机上。
这时候，这个 IP 包的下一个目的地，就取决于宿主机上的路由规则了。此时，Flannel 已经在宿主 机上创建出了一系列的路由规则，以 Node 1 为例，如下所示:

```
# 在 Node 1 上
$ ip route
 default via 10.168.0.1 dev eth0
100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.1.0 5
100.96.1.0/24 dev docker0 proto kernel scope link src 100.96.1.1 6
10.168.0.0/24 dev eth0 proto kernel scope link src 10.168.0.2
```
由于我们的 IP 包的目的地址是 100.96.2.3，它匹配不到本机 docker0 网桥对应的 100.96.1.0/24 网段，只能匹配到第二条、也就是 100.96.0.0/16 对应的这条路由规则，从而进入 到一个叫作 flannel0 的设备中。

而这个 flannel0 设备的类型就比较有意思了:它是一个 **TUN 设备(Tunnel 设备)。**
在 Linux 中，TUN 设备是一种工作在三层(Network Layer)的虚拟网络设备。TUN 设备的功能
非常简单，即:**在操作系统内核和用户应用程序之间传递 IP 包。**

以 flannel0 设备为例:
像上面提到的情况，当操作系统将一个 IP 包发送给 flannel0 设备之后，flannel0 就会把这个 IP 包，交给创建这个设备的应用程序，也就是 Flannel 进程。这是一个从内核态(Linux 操作系统) 向用户态(Flannel 进程)的流动方向。
反之，如果 Flannel 进程向 flannel0 设备发送了一个 IP 包，那么这个 IP 包就会出现在宿主机网络 栈中，然后根据宿主机的路由表进行下一步处理。这是一个从用户态向内核态的流动方向。
所以，当 IP 包从容器经过 docker0 出现在宿主机，然后又根据路由表进入 flannel0 设备后，宿主 机上的 flanneld 进程(Flannel 项目在每个宿主机上的主进程)，就会收到这个 IP 包。然后， flanneld 看到了这个 IP 包的目的地址，是 100.96.2.3，就把它发送给了 Node 2 宿主机。

*flanneld 又是如何知道这个 IP 地址对应的容器，是运行在 Node 2 上的呢?*

这里，就用到了 Flannel 项目里一个非常重要的概念:子网(Subnet)。
事实上，在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一 个“子网”。在我们的例子中，Node 1 的子网是 100.96.1.0/24，container-1 的 IP 地址是 100.96.1.2。Node 2 的子网是 100.96.2.0/24，container-2 的 IP 地址是 100.96.2.3。

而这些子网与宿主机的对应关系，正是保存在 Etcd 当中，如下所示:
```
$ etcdctl ls /coreos.com/network/subnets
/coreos.com/network/subnets/100.96.1.0-24
/coreos.com/network/subnets/100.96.2.0-24
/coreos.com/network/subnets/100.96.3.0-24
```

flanneld 进程在处理由 flannel0 传入的 IP 包时，就可以根据目的 IP 的地址(比如 100.96.2.3)，匹配到对应的子网(比如 100.96.2.0/24)，从 Etcd 中找到这个子网对应的宿主机 的 IP 地址是 10.168.0.3.

lanneld 在收到 container-1 发给 container-2 的 IP 包之后，就会把这个 IP 包直接封装 在一个 UDP 包里，然后发送给 Node 2。不难理解，这个 UDP 包的源地址，就是 flanneld 所在 的 Node 1 的地址，而目的地址，则是 container-2 所在的宿主机 Node 2 的地址。
当然，这个请求得以完成的原因是，每台宿主机上的 flanneld，都监听着一个 8285 端口，所以 flanneld 只要把 UDP 包发往 Node 2 的 8285 端口即可。

通过这样一个普通的、宿主机之间的 UDP 通信，一个 UDP 包就从 Node 1 到达了 Node 2。而 Node 2 上监听 8285 端口的进程也是 flanneld，所以这时候，flanneld 就可以从这个 UDP 包里 解析出封装在里面的、container-1 发来的原 IP 包。
而接下来 flanneld 的工作就非常简单了:flanneld 会直接把这个 IP 包发送给它所管理的 TUN 设 备，即 flannel0 设备。Linux 内核网络栈就会负责处理这个 IP 包，具体的处理方法，就是 通过本机的路由表来寻找这个 IP 包的下一步流向。


Flannel UDP 模式提供的其实是一个三层的 Overlay 网络，即:它首先对发出端的 IP 包进行 UDP 封装，然后在接收端进行解封装拿到原始的 IP 包，进而把这个 IP 包转发给目标容 器。这就好比，Flannel 在不同宿主机上的两个容器之间打通了一条“隧道”，使得这两个容器可 以直接使用 IP 地址进行通信，而无需关心容器和宿主机的分布情况。

相比于两台宿主机之间的直接通信，基于 Flannel UDP 模式的容器通信多了一个额外的步 骤，即 flanneld 的处理过程。而这个过程，由于使用到了 flannel0 这个 TUN 设备，仅在发出 IP 包的过程中，就需要经过三次用户态与内核态之间的数据拷贝:

第一次:用户态的容器进程发出的 IP 包经过 docker0 网桥进入内核态;
第二次:IP 包根据路由表进入 TUN(flannel0)设备，从而回到用户态的 flanneld 进程;
第三次:flanneld 进行 UDP 封包之后重新进入内核态，将 UDP 包通过宿主机的 eth0 发出去。

Flannel 进行 UDP 封装(Encapsulation)和解封装(Decapsulation) 的过程，也都是在用户态完成的。在 Linux 操作系统中，上述这些上下文切换和用户态操作的代价 其实是比较高的，这也正是造成 Flannel UDP 模式性能不好的主要原因。

我们在进行系统级编程的时候，有一个非常重要的优化原则，**就是要减少用户态到内核态 的切换次数，并且把核心的处理逻辑都放在内核态进行**。这也是为什么，Flannel 后来支持的 VXLAN 模式，逐渐成为了主流的容器网络方案的原因。


**VXLAN**

Virtual Extensible LAN(虚拟可扩展局域网)，是 Linux 内核本身就支持的一种网络 虚似化技术。所以说，VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与前面 相似的“隧道”机制，构建出覆盖网络(Overlay Network)。

VXLAN 的覆盖网络的设计思想是:在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”(虚拟机或者容器都可 以)之间，可以像在同一个局域网(LAN)里那样自由通信。当然，实际上，这些“主机”可能分 布在不同的宿主机上，甚至是分布在不同的物理机房里。

而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧 道”的两端。这个设备就叫作 VTEP，即:VXLAN Tunnel End Point(虚拟隧道端点)。

而 VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对 象，是二层数据帧(Ethernet frame);而且这个工作的执行流程，全部是在内核里完成的(因为VXLAN 本身就是 Linux 内核中的一个模块)。

[VNET](files/VNET.png)

图中每台宿主机上名叫 flannel.1 的设备，就是 VXLAN 所需的 VTEP 设备，它既有 IP 地址，也有 MAC 地址。
现在，我们的 container-1 的 IP 地址是 10.1.15.2，要访问的 container-2 的 IP 地址是 10.1.16.3。

与前面 UDP 模式的流程类似，当 container-1 发出请求之后，这个目的地址是 10.1.16.3 的 IP 包，会先出现在 docker0 网桥，然后被路由到本机 flannel.1 设备进行处理。也就是说，来 到了“隧道”的入口。为了方便叙述，我接下来会把这个 IP 包称为“原始 IP 包”。

为了能够将“原始 IP 包”封装并且发送到正确的宿主机，VXLAN 就需要找到这条“隧道”的出 口，即:目的宿主机的 VTEP 设备。

接下来我会把 Node 1 和 Node 2 上的 flannel.1 设备分别称为“源 VTEP 设 备”和“目的 VTEP 设备”。
而这些 VTEP 设备之间，就需要想办法组成一个虚拟的二层网络，即:通过二层数据帧进行通信。

“源 VTEP 设备”收到“原始 IP 包”后，就要想办法把“原始 IP 包”加上 一个目的 MAC 地址，封装成一个二层数据帧，然后发送给“目的 VTEP 设备”(当然，这么做还 是因为这个 IP 包的目的地址不是本机)。

这里需要解决的问题就是:“目的 VTEP 设备”的 MAC 地址是什么?
此时，根据前面的路由记录，我们已经知道了“目的 VTEP 设备”的 IP 地址。而要根据三层 IP 地 址查询对应的二层 MAC 地址，这正是 ARP(Address Resolution Protocol )表的功能。

而这里要用到的 ARP 记录，也是 flanneld 进程在 Node 2 节点启动时，自动添加在 Node 1 上 的。

Linux 内核会把“目的 VTEP 设备”的 MAC 地址，填写在图中的 Inner Ethernet
Header 字段，得到一个二层数据帧。

上述封包过程只是加一个二层头，不会改变“原始 IP 包”的内容。所以图中的 Inner IP Header 字段，依然是 container-2 的 IP 地址，即 10.1.16.3。

上面提到的这些 VTEP 设备的 MAC 地址，对于宿主机网络来说并没有什么实际意义。所以 上面封装出来的这个数据帧，并不能在我们的宿主机二层网络里传输。为了方便叙述，我们把它称 为“内部数据帧”(Inner Ethernet Frame)。

Linux 内核还需要再把“内部数据帧”进一步封装成为宿主机网络里的一个普通的数 据帧，好让它“载着”“内部数据帧”，通过宿主机的 eth0 网卡进行传输。
我们把这次要封装出来的、宿主机对应的数据帧称为“外部数据帧”(Outer Ethernet Frame)。
为了实现这个“搭便车”的机制，Linux 内核会在“内部数据帧”前面，加上一个特殊的 VXLAN 头，用来表示这个“乘客”实际上是一个 VXLAN 要使用的数据帧。
而这个 VXLAN 头里有一个重要的标志叫作VNI，它是 VTEP 设备识别某个数据帧是不是应该归自 己处理的重要标识。而在 Flannel 中，VNI 的默认值是 1，这也是为何，宿主机上的 VTEP 设备都 叫作 flannel.1 的原因，这里的“1”，其实就是 VNI 的值。

Linux 内核会把这个数据帧封装进一个 UDP 包里发出去。
